{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c68ed585-6bab-44f2-b766-9cbea52b5b9f",
   "metadata": {},
   "outputs": [],
   "source": [    
    "############### Pro-IDD : Pareto-based ensemble for imbalanced and drifting data streams ####################\n",
    "################################# Authors: Muhammad Usman, Huanhuan Chen ####################################\n",
    "############################## For any queries: muhammadusman@mail.ustc.edu.cn ##############################\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from skmultiflow.trees import HoeffdingTreeClassifier\n",
    "from time import perf_counter\n",
    "from sklearn import clone\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import statistics\n",
    "from decimal import Decimal\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn import preprocessing\n",
    "from numpy import sqrt\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math \n",
    "from numpy.linalg import norm\n",
    "import statistics\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial  import distance_matrix\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "import pygmo as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pygmo import *\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.8f}'.format)\n",
    "#np.set_printoptions(suppress=True)\n",
    "#np.set_printoptions(threshold=1000000000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def __get_coefficients(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = 0, 0, 0, 0\n",
    "    for i in range(y_true.shape[0]):\n",
    "        if y_pred_a[i] == y_true[i] and y_pred_b[i] == y_true[i]:\n",
    "            a = a + 1\n",
    "        elif y_pred_a[i] != y_true[i] and y_pred_b[i] == y_true[i]:\n",
    "            b = b + 1\n",
    "        elif y_pred_a[i] == y_true[i] and y_pred_b[i] != y_true[i]:\n",
    "            c = c + 1\n",
    "        else:\n",
    "            d = d + 1\n",
    "    return a, b, c, d\n",
    "\n",
    "\n",
    "\n",
    "def agreement_measure(y_true, y_pred_a, y_pred_b):\n",
    "    a, b, c, d = __get_coefficients(y_true, y_pred_a, y_pred_b)\n",
    "    agreement = 1\n",
    "    try:\n",
    "        agreement = (a + b + c + d) / float(b + c)\n",
    "    except:\n",
    "        agreement = 1\n",
    "    return agreement\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string  \n",
    "    for ele in s: \n",
    "        str1 += str(ele)+ \"-\" \n",
    "    \n",
    "    # return string  \n",
    "    return str1 \n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "def pltcolor(lst):\n",
    "    cols=[]\n",
    "    for l in lst:\n",
    "        if l==0:\n",
    "            cols.append('blue')\n",
    "        elif l==1:\n",
    "            cols.append('red')\n",
    "    return cols\n",
    "\n",
    "\n",
    "\n",
    "def detect_overlap(data_min,data_maj,n_features,current_window):\n",
    "    global arr_win_x\n",
    "    fishers = []\n",
    "    found = 0\n",
    "    max_f1 = 0\n",
    "    max_feature_index = 0\n",
    "    \n",
    "   \n",
    "    Xy_up = data_min\n",
    "    Xy_down = data_maj\n",
    "    frames = [Xy_up,Xy_down]\n",
    "    Xy_d = pd.concat(frames)\n",
    "    \n",
    "    f_data_up_means = Xy_up.mean()\n",
    "    f_data_down_means= Xy_down.mean()\n",
    "    \n",
    "    f_data_up_vars = Xy_up.var()\n",
    "    f_data_down_vars =Xy_down.var()\n",
    "    \n",
    "    f2_val = 1\n",
    "    for i in range(n_features):\n",
    "        mean_up = f_data_up_means[i]\n",
    "        mean_down = f_data_down_means[i]\n",
    "        var_up = f_data_up_vars[i]\n",
    "        var_down = f_data_down_vars[i]\n",
    "        fisher = ((mean_up-mean_down)**2) / (var_up + var_down)\n",
    "        fishers.append([i,round(fisher,5),round(mean_up,5),round(mean_down,5)])\n",
    "        if(fisher>max_f1) :\n",
    "            max_f1 = fisher\n",
    "            max_feature_index = i\n",
    "\n",
    "    max_index= max_feature_index + 1\n",
    "    #print(\"Actual Max:\",max_f1)\n",
    "    max_f1 = 1/(1+max_f1)   # push between (0,1]\n",
    "    #print(\"Transformed Max:\",max_f1)       \n",
    "    fishers=sorted(fishers,key=lambda x: x[1], reverse=False)\n",
    "    return round(max_f1,5),round(f2_val,5), fishers\n",
    "\n",
    "\n",
    "def min_plusplus(X_maj,X_min,y_maj,y_min,n_features,overlap_val,ups,downs):\n",
    "        global overlap_threshold_f1\n",
    "        global output_folder\n",
    "        global imbalance_ratio_threshold\n",
    "        \n",
    "        \n",
    "        ##################### Update Imbalance status of current window\n",
    "        is_imbalance = False\n",
    "        ir = 0\n",
    "        if(ups ==0 or downs ==0):\n",
    "            ir = 99 # very high, always considered imbalanced\n",
    "        elif(ups > downs):\n",
    "            ir = ups/downs\n",
    "        elif(ups < downs):\n",
    "            ir = downs/ups\n",
    "        elif(ups == downs):\n",
    "            ir = 1\n",
    "        arr_ir.append(ir)\n",
    "        if(ir>=imbalance_ratio_threshold):\n",
    "            is_imbalance = True\n",
    "            \n",
    "            \n",
    "        ######################################## Execure Min++ functions        \n",
    "        if(is_imbalance == False): #nothing to be done if there is no imbalance            \n",
    "            frames = [X_min,X_maj]\n",
    "            modified_X = pd.concat(frames)\n",
    "            frames = [y_min,y_maj]        \n",
    "            modified_Y = pd.concat(frames)\n",
    "            return modified_X.to_numpy(), modified_Y.to_numpy().reshape(-1)\n",
    "                \n",
    "                        \n",
    "        if(is_imbalance==True): # 1) fix disjuncts and overlaps if there is imbalance\n",
    "            \n",
    "            #Get updated minority data\n",
    "            modified_Xmin,modified_Ymin = fix_disjuncts(X_min,y_min,(len(X_maj)-len(X_min)))\n",
    "            \n",
    "            #see if majority is to be fixed by removing overlaps\n",
    "            modified_Xmaj= X_maj\n",
    "            modified_Ymaj = y_maj\n",
    "            \n",
    "            is_overlap = False            \n",
    "            if (overlap_val > overlap_threshold_f1): \n",
    "                is_overlap =  True\n",
    "\n",
    "            if is_overlap:\n",
    "                modified_Xmaj = modified_Xmaj.reset_index(drop=True)\n",
    "                modified_Xmin = modified_Xmin.reset_index(drop=True)\n",
    "                idx_maj = len(modified_Xmaj)\n",
    "                all_data = pd.concat([modified_Xmaj,modified_Xmin])\n",
    "                a = pdist(all_data,metric='euclidean')\n",
    "                t_l = len(all_data)\n",
    "                d_1 = linkage(a)\n",
    "                d_2 = pd.DataFrame(d_1,columns=['i1','i2','i3','i4'])\n",
    "                filtered_values = np.where( ((d_2[\"i1\"] < idx_maj) & (d_2[\"i2\"] > idx_maj) & (d_2[\"i2\"] <t_l)  ) | ((d_2[\"i1\"] > idx_maj) & (d_2[\"i1\"] < t_l ) & (d_2[\"i2\"] < idx_maj) ) )\n",
    "                filtered =  d_2.loc[filtered_values]\n",
    "                arr_indexes = filtered.iloc[:,0].to_numpy() \n",
    "                to_remove = round(len(arr_indexes)*disjunct_threshold_perc_maj)\n",
    "                modified_Xmaj.drop(index=arr_indexes,inplace=True)\n",
    "                modified_Ymaj = modified_Ymaj.drop(modified_Ymaj.index[:to_remove])\n",
    "                \n",
    "            \n",
    "        #combine minority and majority\n",
    "        frames = [modified_Xmin,modified_Xmaj]\n",
    "        modified_X = pd.concat(frames)\n",
    "        frames = [modified_Ymin,modified_Ymaj]\n",
    "        modified_Y = pd.concat(frames)\n",
    "            \n",
    "        #return modified data\n",
    "        return modified_X.to_numpy(), modified_Y.to_numpy().reshape(-1)\n",
    "\n",
    "def fix_disjuncts(X,y,samples_to_add):\n",
    "    global disjuncts\n",
    "    global output_folder\n",
    "    global disjuncts_threshold\n",
    "    global arr_ord_cols\n",
    "\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "    data_scaled = X\n",
    "    n_samples, n_features = X.shape\n",
    "    try:\n",
    "        data_scaled = preprocessing.scale(X)\n",
    "    except:\n",
    "        return X,y\n",
    "    linked = linkage(data_scaled, 'average')\n",
    "    last = linked[-10:, 2]\n",
    "    last_rev = last[::-1]\n",
    "    idxs = np.arange(1, len(last) + 1)\n",
    "    accele = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "    accele_rev = accele[::-1]\n",
    "    k = accele_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "    #k = random.randint(2,7)\n",
    "    model = AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto', connectivity=None, linkage='average', memory=None, n_clusters=k)\n",
    "    y_hc=model.fit_predict(X)\n",
    "    aggloclust=model.fit(X)\n",
    "    \n",
    "    (unique, counts) = np.unique(aggloclust.labels_, return_counts=True)\n",
    "    frequencies = np.asarray((unique, counts)).T\n",
    "    frequencies = frequencies[np.argsort(frequencies[:, 1])[::-1]]\n",
    "\n",
    "    arr_cumsum = np.cumsum(frequencies,axis=0)\n",
    "    cumsum = arr_cumsum[:,1]\n",
    "    arr_sum_cumsum = np.sum(frequencies,axis=0)\n",
    "    arr_clusters = []\n",
    "    sum_cumsum = arr_sum_cumsum[1]\n",
    "    for i in range(len(arr_cumsum)):\n",
    "        val = cumsum[i]\n",
    "        arr_clusters.append(frequencies[i,0])\n",
    "\n",
    "    \n",
    "    arr_y_frames  = []\n",
    "    arr_x_frames  = []\n",
    "    cluster1_count =0\n",
    "    df_x1 = []\n",
    "    df_y1 = []\n",
    "    disjuncts = len(arr_clusters)\n",
    "    if(disjuncts<disjuncts_threshold):\n",
    "        return X,y\n",
    "    \n",
    "    valid_clusters =0\n",
    "    for i in range(len(arr_clusters)):\n",
    "        arr_indexes = ((np.where(y_hc == int(arr_clusters[i]))))\n",
    "        df_y = y.loc[y.index[arr_indexes]]\n",
    "        if(len(df_y) > 2):\n",
    "            valid_clusters += 1\n",
    "    if(valid_clusters == 0):\n",
    "        return X,y\n",
    "    each_cluster_addition = int(np.round(samples_to_add / valid_clusters ))\n",
    "    \n",
    "    for i in range(len(arr_clusters)):\n",
    "            arr_indexes = ((np.where(y_hc == int(arr_clusters[i]))))\n",
    "            df_x = X.loc[X.index[arr_indexes]]\n",
    "            df_y = y.loc[y.index[arr_indexes]]\n",
    "            \n",
    "            \n",
    "            if(len(df_x) <3): #could be outliers + avoid exception in sample generation\n",
    "                continue\n",
    "            \n",
    "            to_add = 0\n",
    "            if(i==0):\n",
    "                cluster1_count = len(df_x)\n",
    "                to_add = each_cluster_addition                \n",
    "            else:\n",
    "                to_add = each_cluster_addition\n",
    "                \n",
    "                #make equal size clusters in minority\n",
    "                current_cluster_count = len(df_x)                    \n",
    "                to_add += (cluster1_count-current_cluster_count)\n",
    "            \n",
    "            df_cov = df_x.cov()\n",
    "            df_mean = df_x.mean(axis=0)\n",
    "            new_samples = np.random.multivariate_normal(df_mean, df_cov,to_add)\n",
    "            \n",
    "            cols = []\n",
    "            for f in range(1,n_features+1):\n",
    "                cols.append(str(f))\n",
    "            df_new_samples = pd.DataFrame(new_samples,columns=cols)\n",
    "            df_x = pd.concat([df_x.reset_index(drop=True),df_new_samples.reset_index(drop=True)],axis=0)\n",
    "            df_x = df_x.reset_index(drop=True)\n",
    "            \n",
    "            #round values for ordinal cols\n",
    "            if(len(arr_ord_cols)>0):\n",
    "                for h in range (0,len(arr_ord_cols)):\n",
    "                    ix = arr_ord_cols[h]\n",
    "                    df_x[str(ix)] = round(df_x[str(ix)]) \n",
    "            for j in range(0,to_add):\n",
    "                df_y = df_y.append(pd.Series([1],index=['class']),ignore_index = True) \n",
    "            df_y = df_y.reset_index(drop=True)\n",
    "            \n",
    "            arr_x_frames.append(df_x)\n",
    "            arr_y_frames.append(df_y)\n",
    "        \n",
    "    \n",
    "    np.set_printoptions(threshold=np.inf)            \n",
    "    modified_X = pd.concat(arr_x_frames)\n",
    "    modified_Y = pd.concat(arr_y_frames)\n",
    "    \n",
    "    modified_X = modified_X.reset_index(drop=True)\n",
    "    modified_Y = modified_Y.reset_index(drop=True)\n",
    "\n",
    "    return modified_X,modified_Y    \n",
    "\n",
    "\n",
    "class DetectorClassifier(BaseEstimator):\n",
    "    def __init__(self, clf, classes):\n",
    "        if not hasattr(clf, \"partial_fit\"):\n",
    "            raise TypeError(\"Choose incremental classifier\")\n",
    "        self.clf = clf\n",
    "        \n",
    "        self.classes = classes\n",
    "        self.change_detected = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #self.clf = clone(self.clf)\n",
    "        self.clf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self,modified_X,modified_Y,fullFit):\n",
    "            if(fullFit):\n",
    "               # print(\"^^^^Full Fit^^^\")    \n",
    "                self.clf = self.clf.fit(modified_X, modified_Y, classes=self.classes)            \n",
    "            else:\n",
    "               # print(\"^^^^Partial Fit^^^\")\n",
    "                self.clf = self.clf.partial_fit(modified_X, modified_Y, classes=self.classes)\n",
    "\n",
    "            return \"\", self\n",
    "    \n",
    "    ##### Prdict Function        \n",
    "    def predict(self, X):\n",
    "        return np.asarray(self.clf.predict(X), dtype=np.int64).ravel()\n",
    "    \n",
    "    ##### Score Function\n",
    "    def score(self, X,y):\n",
    "        return self.clf.score(X,y)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X) \n",
    "    \n",
    "    \n",
    "    \n",
    "def prequential(window_size,Xy,X, y, n_features,file_out_prefix):\n",
    "    \n",
    "    \"\"\"Prequential Evaluation: First test the incoming instances and prepare a batch to train the model again.\n",
    "    \"\"\"\n",
    "        \n",
    "    arr_clf = []\n",
    "    arr_clf_temp_to_use = []\n",
    "    arr_weights = []\n",
    "    arr_recall0_cw = []\n",
    "    arr_recall1_cw = []\n",
    "    \n",
    "    a= 0.00001\n",
    "    \n",
    "\n",
    "    global arr_recall_0\n",
    "    global arr_recall_1\n",
    "    global output_folder    \n",
    "    global arr_recall\n",
    "    global arr_specificity\n",
    "    global maj_weight\n",
    "    global min_weight\n",
    "    global overlap_threshold\n",
    "    global imbalance_ratio_threshold\n",
    "    global disjunct_threshold_perc_maj\n",
    "    global pareto_indices\n",
    "    global ensemble_pool_size\n",
    "    \n",
    "    pareto_indices = []\n",
    "    arr_actual_win = []\n",
    "    arr_pred_win = []\n",
    "    arr_roc = []\n",
    "    arr_divs = []\n",
    "    ensemble_tp = 0\n",
    "    ensemble_fp =0\n",
    "    ensemble_fn = 0\n",
    "    ensemble_tn = 0\n",
    "    disjuncts = 0\n",
    "    \n",
    "    ensemble_correct =0\n",
    "    zeroClassCounter = 0\n",
    "    oneClassCounter = 0\n",
    "    current_correct_0 = 0\n",
    "    current_correct_1 = 0\n",
    "    \n",
    "    current_correct_ensemble_0 = 0\n",
    "    current_correct_ensemble_1 = 0\n",
    "    \n",
    "    predictions = \"\"\n",
    "    \n",
    "    \n",
    "    row_num = y.shape[0]\n",
    "    \n",
    "    # Split an init batch for training of same size as of batch\n",
    "    X_init = X[0:w]\n",
    "    y_init = y[0:w]\n",
    "    \n",
    "    # Used for prequentail testing and training\n",
    "    X_train = X[0:]\n",
    "    y_train = y[0:]    \n",
    "    Xy_train = Xy[0:]\n",
    "    \n",
    "    #Due to K=3 in Equation 6. If more histor is required, add more variables here. \n",
    "    #Non Dynamic Code\n",
    "    recall_0_history_1 = np.zeros(ensemble_pool_size)\n",
    "    recall_0_history_2 = np.zeros(ensemble_pool_size)\n",
    "    recall_0_history_3 = np.zeros(ensemble_pool_size)\n",
    "    recall_1_history_1 = np.zeros(ensemble_pool_size)\n",
    "    recall_1_history_2 = np.zeros(ensemble_pool_size)\n",
    "    recall_1_history_3 = np.zeros(ensemble_pool_size)\n",
    "    \n",
    "    \n",
    "    ensemble_preds = np.zeros(row_num-w)\n",
    "    current_correct_0_cw = np.zeros(ensemble_pool_size)\n",
    "    current_correct_1_cw = np.zeros(ensemble_pool_size)\n",
    "    recall_0_cw = np.zeros(ensemble_pool_size)\n",
    "    recall_1_cw = np.zeros(ensemble_pool_size)\n",
    "    \n",
    "    \n",
    "    arr_temp = []\n",
    "\n",
    "    \n",
    "    #Create classifiers and add to the pool trained on initial batch\n",
    "    for g in range (0,1):\n",
    "        obj_clf = DetectorClassifier(HoeffdingTreeClassifier(), np.unique(y))\n",
    "        obj_clf = obj_clf.fit(X_init, y_init)\n",
    "        arr_clf.append(obj_clf)\n",
    "        arr_weights.append(1) \n",
    "        pareto_indices=np.append(pareto_indices,0) \n",
    "        \n",
    "    current_w = 0\n",
    "    window_counter = 0\n",
    "    global prev_win_x \n",
    "    global prev_win_y \n",
    "    strProcessing = \"\"\n",
    "    correct_predictions = 0\n",
    "\n",
    "    strout = \"\"\n",
    "    strgmean = \"\"\n",
    "\n",
    "    oneClassCounter_CWindow = 0\n",
    "    zeroClassCounter_CWindow = 0\n",
    "    arr_cw_ens_preds = []\n",
    "    current_correct_0_cw_est = 0\n",
    "    current_correct_1_cw_est = 0\n",
    "    window_recall_0 = 0\n",
    "    window_recall_1 = 0\n",
    "    arr_accuracy = []\n",
    "    \n",
    "    current_ensemble = []\n",
    "    current_weights = []\n",
    "    current_ensemble_use = []\n",
    "    \n",
    "    weights_cal_flag = True  #Send true first time to prepare default weights\n",
    "    \n",
    "    for i in range(0, row_num - n_train):\n",
    "        arr_x.append(i)\n",
    "        current_w += 1\n",
    "        start_time = perf_counter()\n",
    "        \n",
    "        arr_preds = []\n",
    "        arr_temp_preds = []\n",
    "        \n",
    "        #predict using each base classifier to calculate recalls\n",
    "        for n in range(0,len(arr_clf)):\n",
    "            clf = arr_clf[n]\n",
    "            prediction_clf = clf.predict(X_train[i, :].reshape(1, -1))\n",
    "            arr_preds.append(prediction_clf[0])\n",
    "            arr_temp_preds.append(prediction_clf[0])\n",
    "        \n",
    "        arr_cw_ens_preds.append(arr_temp_preds)            \n",
    "       \n",
    "        #weights are re-calculated only if batch is ready\n",
    "        #the flag is set upon start of training, and once the batch is ready\n",
    "        if( weights_cal_flag == True):            \n",
    "            weights_cal_flag = False ## no repeat unless new batch is ready\n",
    "            \n",
    "            df_weights_arr = pd.DataFrame(arr_weights)            \n",
    "            df_weights_arr = df_weights_arr.sort_values(ascending=False,by=df_weights_arr.columns[0])\n",
    "            arr_clf_temp_to_use = []\n",
    "            estimators_arr_temp = []\n",
    "            arr_weights_temp = []\n",
    "                \n",
    "            if(len(arr_clf) < ensemble_pool_size):\n",
    "                for b in range (0,len(arr_clf)):\n",
    "                    arr_clf_temp_to_use.append(arr_clf[b])\n",
    "                    estimators_arr_temp.append((\"HT-\" + str(b), arr_clf[b]))\n",
    "                    arr_weights_temp.append(1)     \n",
    "            else:                    \n",
    "                ## get weights of preto-selected classifiers\n",
    "                for h in range(0,len(pareto_indices)) :\n",
    "                    try:\n",
    "                        indexx = pareto_indices[h].astype(int)\n",
    "                    except:\n",
    "                        indexx = pareto_indices[h]   \n",
    "                    index_original = df_weights_arr.iloc[[h]].index.values[0]            \n",
    "                    arr_clf_temp_to_use.append(arr_clf[indexx])\n",
    "                    estimators_arr_temp.append((\"HT-\" + str(index_original), arr_clf[indexx]))\n",
    "                    arr_weights_temp.append(arr_weights[indexx])        \n",
    "            \n",
    "            \n",
    "        \n",
    "        #Ensemble Prediction\n",
    "        pred = 0\n",
    "        eclf = VotingClassifier(estimators = estimators_arr_temp, voting='hard', weights=arr_weights_temp) \n",
    "        eclf.estimators_ = arr_clf_temp_to_use\n",
    "        eclf.le_ = LabelEncoder().fit(np.unique([0, 1]))\n",
    "        pred = eclf.predict(X_train[i, :].reshape(1, -1))\n",
    "        arr_pred_win.append(pred)\n",
    "        \n",
    "        \n",
    "        lambda_0 = maj_weight #Majority\n",
    "        lambda_1 = min_weight #Minority\n",
    "        \n",
    "        if y_train[i] == 0:\n",
    "            zeroClassCounter += 1\n",
    "            zeroClassCounter_CWindow += 1\n",
    "        else: \n",
    "            oneClassCounter += 1\n",
    "            oneClassCounter_CWindow += 1\n",
    "        \n",
    "        \n",
    "        for n in range(0,len(arr_clf)):\n",
    "            if(y_train[i] == 0 and arr_preds[n] ==0):\n",
    "                current_correct_0_cw[n] = current_correct_0_cw[n]+1\n",
    "            if(y_train[i] == 1 and arr_preds[n] ==1):    \n",
    "                current_correct_1_cw[n] = current_correct_1_cw[n]+1\n",
    "        \n",
    "        correct_found = 0  \n",
    "        y_actual = 0\n",
    "        y_pred = 0\n",
    "        if(y_train[i] ==0 and pred ==0):\n",
    "            correct_found +=1\n",
    "            current_correct_ensemble_0 += 1\n",
    "            current_correct_0_cw_est += 1\n",
    "            ensemble_tn += 1\n",
    "            y_actual = 0\n",
    "            y_pred = 0\n",
    "        if(y_train[i] ==1 and pred == 1):\n",
    "            correct_found +=1\n",
    "            current_correct_ensemble_1 += 1\n",
    "            current_correct_1_cw_est += 1\n",
    "            ensemble_tp += 1\n",
    "            y_actual = 1\n",
    "            y_pred = 1\n",
    "        if(y_train[i] ==1 and pred ==0):\n",
    "            ensemble_fn += 1\n",
    "            y_actual = 1\n",
    "            y_pred = 0\n",
    "        if(y_train[i] ==0 and pred == 1):\n",
    "            ensemble_fp += 1\n",
    "            y_actual = 0\n",
    "            y_pred = 1\n",
    "        if(correct_found ==1):\n",
    "            ensemble_correct += 1\n",
    "            \n",
    "        arr_actual_win.append(y_actual)\n",
    "        \n",
    "        #when the batch is ready or an in complete batch at the end\n",
    "        if( ( current_w % window_size == 0)  or (current_w != window_size and i == (row_num - w -1)) ):\n",
    "            arr_win_x.append(window_counter)\n",
    "            window_counter += 1\n",
    "            \n",
    "            ##window-based evaluation measures calculation\n",
    "            window_recall_0 = 0\n",
    "            window_recall_1 = 0\n",
    "            if zeroClassCounter_CWindow != 0:\n",
    "                window_recall_0 =  current_correct_0_cw_est / zeroClassCounter_CWindow\n",
    "            if oneClassCounter_CWindow != 0:\n",
    "                window_recall_1 =  current_correct_1_cw_est / oneClassCounter_CWindow\n",
    "            \n",
    "            window_gmean = sqrt(window_recall_0*window_recall_1)\n",
    "            arr_gmean.append(window_gmean*100)\n",
    "            arr_recall_0.append(window_recall_0*100)\n",
    "            arr_recall_1.append(window_recall_1*100)            \n",
    "            \n",
    "            specificity = 0\n",
    "            recall = 0\n",
    "            try:\n",
    "                specificity = ensemble_tn/(ensemble_tn+ensemble_fp)\n",
    "            except:\n",
    "                specificity = 0                        \n",
    "            try:\n",
    "                recall = ensemble_tp/(ensemble_tp+ensemble_fn)\n",
    "            except:\n",
    "                recall = 0\n",
    "            \n",
    "            arr_specificity.append(specificity*100)\n",
    "            arr_recall.append(recall*100)\n",
    "                      \n",
    "            ## Historical Recalls \n",
    "            if zeroClassCounter_CWindow != 0:\n",
    "                for n in range(0,len(arr_clf)):\n",
    "                    recall_0_cw[n] =  current_correct_0_cw[n] / zeroClassCounter_CWindow                      \n",
    "                    if(window_counter>2): #Due to K=3 in Equation 6\n",
    "                        recall_0_history_1[n] = recall_0_history_2[n]\n",
    "                        recall_0_history_2[n] = recall_0_history_3[n]\n",
    "                        recall_0_history_3[n] = recall_0_cw[n]\n",
    "                    current_correct_0_cw[n] =0    \n",
    "            if oneClassCounter_CWindow != 0:\n",
    "                for n in range(0,len(arr_clf)):\n",
    "                    recall_1_cw[n] =  current_correct_1_cw[n] / oneClassCounter_CWindow\n",
    "                    if(window_counter>2):#Due to K=3 in Equation 6\n",
    "                        recall_1_history_1[n] = recall_1_history_2[n]\n",
    "                        recall_1_history_2[n] = recall_1_history_3[n]\n",
    "                        recall_1_history_3[n] = recall_1_cw[n]                        \n",
    "                    current_correct_1_cw[n] =0\n",
    "                    \n",
    "            arr_weights_temp_out=[]\n",
    "            arr_diversity_temp_out = []\n",
    "            arr_recall_0_temp_out = []\n",
    "            arr_recall_1_temp_out  = []\n",
    "            arr_final_weight_temp_out = []            \n",
    "            \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                weight = 0\n",
    "                if(window_counter < 3): #for first 2 windows only\n",
    "                    weight = (lambda_0 * recall_0_cw[n]) + (lambda_1*recall_1_cw[n])\n",
    "                else: #window 3 onwards\n",
    "                    #0.2,0.3,0.5 are used in Equation 6 for K = 3. For different values of K, change this code\n",
    "                    recall_0_hist_value =  0.2*recall_0_history_1[n]+  0.3*recall_0_history_2[n]+  0.5*recall_0_history_3[n]\n",
    "                    recall_1_hist_value =  0.2*recall_1_history_1[n]+  0.3*recall_1_history_2[n]+  0.5*recall_1_history_3[n]\n",
    "                    weight = (lambda_0 * recall_0_hist_value) + (lambda_1*recall_1_hist_value)\n",
    "                \n",
    "                arr_weights[n] = weight                \n",
    "                arr_weights_temp_out.append(weight)                \n",
    "                arr_recall_0_temp_out.append(recall_0_cw[n])\n",
    "                arr_recall_1_temp_out.append(recall_1_cw[n])\n",
    "                arr_final_weight_temp_out.append(arr_weights[n])\n",
    "            \n",
    "            \n",
    "            '''### diversity work '''\n",
    "            arr_temp = np.vstack(arr_cw_ens_preds)            \n",
    "            arr_divs = []            \n",
    "            for m in range(0,len(arr_clf)):\n",
    "                a_sum = 0\n",
    "                for n in range(0,len(arr_clf)):\n",
    "                    if(m == n):\n",
    "                        continue\n",
    "                    ens_preds_a = arr_temp[:,m]\n",
    "                    ens_preds_b = arr_temp[:,n]\n",
    "                    agr = 1\n",
    "                    try:\n",
    "                        agr = agreement_measure(y_preq, ens_preds_a, ens_preds_b)\n",
    "                    except: \n",
    "                        agr = 1\n",
    "                    a_sum += agr\n",
    "                if(len(arr_clf) != 1):\n",
    "                    #average over all \n",
    "                    avg = (2*a_sum)/((len(arr_clf))*(len(arr_clf)-1))                    \n",
    "                else:\n",
    "                    avg=1\n",
    "                arr_divs.append(avg)                \n",
    "            '''### diversity work ends here '''\n",
    "            \n",
    "            arr_cw_ens_preds = []            \n",
    "            oneClassCounter_CWindow = 0\n",
    "            zeroClassCounter_CWindow = 0  \n",
    "            current_correct_0_cw_est = 0\n",
    "            current_correct_1_cw_est = 0\n",
    "            ensemble_tn = 0\n",
    "            ensemble_tp = 0\n",
    "            ensemble_fp = 0\n",
    "            ensemble_fn = 0\n",
    "            \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                current_correct_0_cw[n]=0\n",
    "                current_correct_1_cw[n] = 0\n",
    "            \n",
    "            \n",
    "            #print(\"**************************************************************************************\")\n",
    "            print(\"\\nCurrent Window:\", window_counter )\n",
    "            #print(\"From:\",str(i-(window_size-1)))\n",
    "            #print(\"To:\",str(i))\n",
    "            #print(\"Current Ensemble Size:\",len(arr_clf))\n",
    "            #print(\"*******\")\n",
    "            \n",
    "            x_preq = X_train[i-(window_size-1):i]\n",
    "            y_preq = y_train[i-(window_size-1):i]\n",
    "\n",
    "            Xy_preq = Xy_train[i-(window_size-1):i]\n",
    "            ups =  np.count_nonzero(y_preq == 1)\n",
    "            downs = np.count_nonzero(y_preq == 0)\n",
    "            \n",
    "            arr_clf_temp = []\n",
    "\n",
    "            ####################### <Overlap detection> ############################################\n",
    "            cols_str = []\n",
    "            for f in range(1,n_features+1):\n",
    "                cols_str.append(str(f))\n",
    "            cols_str.append('class')\n",
    "            Xy_d = pd.DataFrame(Xy_preq, columns = cols_str)\n",
    "\n",
    "\n",
    "            if(ups < downs):\n",
    "                Xy_min = Xy_d[(Xy_d['class'] == 1)]\n",
    "                Xy_maj = Xy_d[(Xy_d['class'] == 0)]\n",
    "            if(ups>downs):\n",
    "                Xy_min = Xy_d[(Xy_d['class'] == 0 )]\n",
    "                Xy_maj = Xy_d[(Xy_d['class'] == 1 )]\n",
    "\n",
    "            cols = []\n",
    "            for f in range(0,n_features):\n",
    "                cols.append(f)\n",
    "            cols_class = [n_features]\n",
    "            X_min = Xy_min[Xy_min.columns[cols]]\n",
    "            X_maj = Xy_maj[Xy_maj.columns[cols]]\n",
    "            y_min = Xy_min[Xy_min.columns[cols_class]]\n",
    "            y_maj = Xy_maj[Xy_maj.columns[cols_class]]\n",
    "            \n",
    "            frames = [X_min,X_maj]\n",
    "            modified_X = pd.concat(frames)\n",
    "            frames = [y_min,y_maj]\n",
    "            modified_Y = pd.concat(frames)\n",
    "\n",
    "            cols_num = []            \n",
    "            for p in range(0,n_features):\n",
    "                cols_num.append(p)\n",
    "            m_X = modified_X\n",
    "            m_Y = modified_Y            \n",
    "            m_X.columns = cols_num\n",
    "            m_Y.columns = [0]\n",
    "            overlap_val,f2_val, fishers = detect_overlap(X_min, X_maj,n_features,window_counter)   \n",
    "            ####################### </ Overlap Detection> ############################################\n",
    "            \n",
    "            ####################### Balance this batch using Min++ ###################################\n",
    "            mod_x,mod_y = min_plusplus(X_maj,X_min,y_maj,y_min,n_features,overlap_val,ups,downs)                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            ## Partial Fit the previous classifiers                        \n",
    "            arr_temp_clfs = []   \n",
    "            for n in range(0,len(arr_clf)):\n",
    "                clf = arr_clf[n]                    \n",
    "                status,clf_temp = clf.partial_fit(mod_x,mod_y,False)                \n",
    "                arr_temp_clfs.append(clf_temp)\n",
    "            arr_clf = arr_temp_clfs\n",
    "            \n",
    "            \n",
    "            ######################## Pareto Calculation ##################################\n",
    "            points_for_preto = []\n",
    "            for n in range(0,len(arr_clf)):\n",
    "                points_for_preto.append([-1*arr_weights[n],arr_divs[n]])            \n",
    "            pareto_indices = []            \n",
    "            pts = pg.non_dominated_front_2d(points =points_for_preto)\n",
    "            \n",
    "            for r in range (0,len(pts)):\n",
    "                pareto_indices.append(pts[r])\n",
    "            #print(\"diversities:\",arr_divs)\n",
    "            #print(\"weights:\",arr_weights)\n",
    "            #print(\"pareto indices:\",pareto_indices)    \n",
    "            \n",
    "            #remove one classifier if the pool has reached.. \n",
    "            if(len(arr_clf) > ensemble_pool_size-1):\n",
    "                preto_min_index = 0\n",
    "                \n",
    "                least_weight = arr_weights[0]\n",
    "                for q in range (1,len(arr_weights)):\n",
    "                    w_q = arr_weights[q]\n",
    "                    if(w_q < least_weight):                        \n",
    "                        preto_min_index = q\n",
    "                        least_weight = w_q\n",
    "                \n",
    "\n",
    "                arr_clf = np.delete(arr_clf,preto_min_index)\n",
    "                arr_weights = np.delete(arr_weights,preto_min_index)\n",
    "                arr_divs = np.delete(arr_divs,preto_min_index)\n",
    "                for g in range (preto_min_index,len(arr_clf)):\n",
    "                    recall_0_history_1[g] =  recall_0_history_1[g+1]\n",
    "                    recall_0_history_2[g] = recall_0_history_2[g+1]\n",
    "                    recall_0_history_3[g] = recall_0_history_3[g+1]\n",
    "                    recall_1_history_1[g] = recall_1_history_1[g+1]\n",
    "                    recall_1_history_2[g] = recall_1_history_2[g+1]\n",
    "                    recall_1_history_3[g] = recall_1_history_3[g+1]\n",
    "                    current_correct_0_cw[g] = current_correct_0_cw[g+1] \n",
    "                    current_correct_1_cw[g] =  current_correct_1_cw[g+1]\n",
    "                        \n",
    "            \n",
    "            ######################### add new classifier to the pool #####################\n",
    "            obj_clf1 = DetectorClassifier(HoeffdingTreeClassifier(), np.unique(y))\n",
    "            status,clf_temp = obj_clf1.partial_fit(mod_x,mod_y,True) # True here means FULL FIT\n",
    "            arr_weights = np.append(arr_weights,0)\n",
    "            arr_divs = np.append(arr_divs,1)\n",
    "            arr_clf = np.append(arr_clf,clf_temp)\n",
    "            \n",
    "            current_w = 0\n",
    "            prev_win_x = x_preq\n",
    "            prev_win_y =  y_preq\n",
    "            weights_cal_flag = True\n",
    "            strgmean = \"\"\n",
    "            print(\"---\\n Avg. Gmean: \", np.mean(arr_gmean))\n",
    "            print(\"Avg. Specificity: \", np.mean(arr_specificity))\n",
    "            print(\"Avg. Recall: \", np.mean(arr_recall))\n",
    "\n",
    "    #uncomment this line to output the file with specificity, recall and gmeans\n",
    "    #np.savetxt(output_folder + file_out_prefix + \"-\" + str(window_counter) +'.csv',  np.c_[arr_recall,arr_specificity,arr_gmean], delimiter=',',fmt='%1.2f')\n",
    "\n",
    "    return np.mean(arr_gmean),np.mean(arr_recall),np.mean(arr_specificity)\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    data = df.values\n",
    "    return data, data[:, :-1], data[:, -1]\n",
    "\n",
    "def delete_files_from_folder(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "            \n",
    "#*********************************  Main ******************************** #\n",
    "global arr_overlap \n",
    "global imbalance_threshold\n",
    "global arr_recall_0\n",
    "global arr_recall_1\n",
    "global arr_gmean\n",
    "global f1_overlap_value\n",
    "global maj_weight\n",
    "global min_weight\n",
    "global arr_ord_cols\n",
    "global arr_specificity\n",
    "global arr_recall\n",
    "global output_folder\n",
    "global stream_folder\n",
    "global pareto_indices\n",
    "global ensemble_pool_size\n",
    "\n",
    "f1_overlap_value = 0\n",
    "\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "output_folder = cwd +  '/output/'\n",
    "stream_folder = cwd +  '/streams/'\n",
    "#delete_files_from_folder(output_folder)\n",
    "\n",
    "arr_recall = []\n",
    "arr_specificity = []\n",
    "arr_gmean = []\n",
    "arr_recall_0 = []\n",
    "arr_recall_1 = []\n",
    "arr_recall_0_nb = []\n",
    "arr_recall_1_nb = []\n",
    "arr_x = []\n",
    "\n",
    "\n",
    "ensemble_pool_size = 15  #pool size to be used\n",
    "w =1500  #window/batch size\n",
    "overlap_threshold_f1 =0.5 #overlap threshold for detection\n",
    "disjuncts_threshold =2 #min minority disjuncts to consider\n",
    "disjunct_threshold_perc_maj = 0.5 #overlap removal threshold for majority\n",
    "maj_weight = 0.5 #weight for majority class recall in ensemble\n",
    "min_weight = 0.5 #weight for minority class recall in ensemble\n",
    "\n",
    "arr_ord_cols= []\n",
    "arr_ord_cols = [6,7,8,9,10] #1-based index  GMSC\n",
    "#arr_ord_cols = [13,14,15,16,17,18,19,20,21] #1-based index  IJCNN1\n",
    "#arr_ord_cols = [10,11,12] #1-based index  LOAN\n",
    "#arr_ord_cols = [9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28] #1-based index  PKADD\n",
    "\n",
    "\n",
    "#real world data sets\n",
    "#filename = \"ijcnn1-full-30oct2022.csv\"\n",
    "#filename = \"cod-rna-311022.csv\"\n",
    "#filename = \"MiniBooNE_PID_Mod.csv\"\n",
    "filename =  \"GMSC.csv\"\n",
    "#filename = \"loan_191122.csv\"\n",
    "#filename = \"PAKDD.csv\"\n",
    "\n",
    "#synthetic data sets \n",
    "#filename = \"v5_imb.csv\"\n",
    "#filename = \"v9_imb.csv\"\n",
    "#filename = \"moa-sea-50K12-g.csv\"\n",
    "#filename = \"moa-sea-50K12-a.csv\"\n",
    "#filename = \"moa-sea-50K12-gr.csv\"\n",
    "#filename = \"moa-sea-50K12-ar.csv\"\n",
    "#filename = \"moa-sine-50K12-g.csv\"\n",
    "#filename = \"moa-sine-50K12-a.csv\"\n",
    "#filename = \"moa_sine_5k_12.csv\"\n",
    "#filename = \"moa_sine_abrupt_5k_12.csv\"\n",
    "\n",
    "filename = stream_folder  + filename\n",
    "\n",
    "Xy, X, y = read_data(filename)\n",
    "# Set x,y as numeric\n",
    "X = X.astype(float)\n",
    "n_samples, n_features = X.shape\n",
    "print(\"samples:\",n_samples)\n",
    "print(\"features:\",n_features)\n",
    "ups =  np.count_nonzero(y == 1)\n",
    "downs = np.count_nonzero(y == 0)\n",
    "print(\"1s:\",str(ups/n_samples))\n",
    "print(\"0s:\",str(downs/n_samples))\n",
    "\n",
    "arr_gmean_iteration = []\n",
    "arr_recall_iteration = []\n",
    "arr_specificity_iteration = []\n",
    "arr_times = []\n",
    "for k in range (0,10):\n",
    "    arr_specificity = []\n",
    "    arr_recall = [] \n",
    "    arr_recall_0= []\n",
    "    arr_recall_1= []\n",
    "    arr_gmean= []\n",
    "    print(\"------------------------------------------------*******************------------------------------------------\")\n",
    "    print(\"iteration no:\",k)\n",
    "    for i in range(len(clfs)):\n",
    "        file_out_prefix = \"ijcnn1-pareto-\" + str(ensemble_pool_size) + \"-\" + str(maj_weight) + \"-\" + str(min_weight) + \"-\" + str(disjuncts_threshold) + \"-\" + str(overlap_threshold_f1) + \"-\" + str(disjunct_threshold_perc_maj)\n",
    "\n",
    "        start = timer()\n",
    "        file_out_prefix1 = file_out_prefix + \"-\" + str(k)\n",
    "        gmean,recall,specificity = prequential(w,Xy,X, y, n_features,file_out_prefix1)\n",
    "        arr_gmean_iteration.append(gmean)\n",
    "        arr_recall_iteration.append(recall)            \n",
    "        arr_specificity_iteration.append(specificity)\n",
    "        end = timer()\n",
    "        print(\"time:\",end-start)\n",
    "        arr_times.append(end-start)\n",
    "    \n",
    "   \n",
    "\n",
    "print(\"---Iterations Averages---\")\n",
    "print(\"Avg. Gmean: \",np.mean(arr_gmean_iteration))\n",
    "print(\"Avg. Recall: \",np.mean(arr_recall_iteration))\n",
    "print(\"Avg. Specificity: \",np.mean(arr_specificity_iteration))\n",
    "print(\"Avg. Time: \",np.mean(arr_times))\n",
    "sys.exit()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fd5e4-6175-4ff9-976f-bf9923c764a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
